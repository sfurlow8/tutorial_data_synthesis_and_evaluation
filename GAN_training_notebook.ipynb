{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0511634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount to my Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183bf555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/content/drive/MyDrive/mimic_synthetic_data/’: File exists\n",
      "cp: cannot stat 'C:\\Users\\FURLOSX3\\OneDrive - Abbott\\Synthetic Data\\MIMIC_tutorial\\tutorial_data_synthesis_and_evaluation\\Data\\preprocessing\\test\\normalized_testing_data.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp \"C:\\Users\\FURLOSX3\\OneDrive - Abbott\\Synthetic Data\\MIMIC_tutorial\\tutorial_data_synthesis_and_evaluation\\Data\\preprocessing\\test\\normalized_testing_data.csv\" /content/drive/MyDrive/mimic_synthetic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c992a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc9ecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 16 17:34:20 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0             42W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9c300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) # should be true\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0)) # should be NVIDIA A100-SXM4-40GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d430dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements_v2.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_v2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33dc73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'tutorial_data_synthesis_and_evaluation'...\n",
      "remote: Enumerating objects: 172, done.\u001b[K\n",
      "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
      "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
      "remote: Total 172 (delta 28), reused 27 (delta 4), pack-reused 96 (from 1)\u001b[K\n",
      "Receiving objects: 100% (172/172), 986.21 KiB | 12.64 MiB/s, done.\n",
      "Resolving deltas: 100% (81/81), done.\n",
      "/content/tutorial_data_synthesis_and_evaluation\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/sfurlow8/tutorial_data_synthesis_and_evaluation.git\n",
    "%cd tutorial_data_synthesis_and_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab379544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.10.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0, 2.21.0rc0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.10.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701eef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97830946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob2onehot(prob):\n",
    "    return tf.cast((tf.reduce_max(prob, axis=-1, keepdims=True) - prob) == 0, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator NN for GAN\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, parameter_dict):\n",
    "        super(Generator, self).__init__()\n",
    "        self.G_DIMS = [parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['dimension']-parameter_dict['race_dimension']]\n",
    "        self.dense_layers = [tf.keras.layers.Dense(dim) for dim in self.G_DIMS[:-1]]\n",
    "        self.batch_norm_layers = [tf.keras.layers.BatchNormalization(epsilon=1e-5) for _ in self.G_DIMS[:-1]]\n",
    "        self.output_layer_code = tf.keras.layers.Dense(self.G_DIMS[-1], activation=tf.nn.sigmoid)\n",
    "        self.output_layer_race = tf.keras.layers.Dense(parameter_dict['race_dimension'], activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        h = self.dense_layers[0](x)\n",
    "        x = tf.nn.relu(self.batch_norm_layers[0](h, training=training))\n",
    "        for i in range(1,len(self.G_DIMS[:-1])):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = tf.nn.relu(self.batch_norm_layers[i](h, training=training))\n",
    "            x += h\n",
    "        x = tf.concat((self.output_layer_race(x), self.output_layer_code(x)),axis=-1)\n",
    "        return x\n",
    "\n",
    "    def test(self, x):\n",
    "        h = self.dense_layers[0](x)\n",
    "        x = tf.nn.relu(self.batch_norm_layers[0](h, training=False))\n",
    "        for i in range(1,len(self.G_DIMS[:-1])):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = tf.nn.relu(self.batch_norm_layers[i](h, training=False))\n",
    "            x += h\n",
    "        x = tf.concat((prob2onehot(self.output_layer_race(x)), self.output_layer_code(x)),axis=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator NN for GAN\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, parameter_dict):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.D_DIMS = [parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension']]\n",
    "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.relu) for dim in self.D_DIMS]\n",
    "        self.layer_norm_layers = [tf.keras.layers.LayerNormalization(epsilon=1e-5) for _ in self.D_DIMS]\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense_layers[0](x)\n",
    "        x = self.layer_norm_layers[0](x)\n",
    "        for i in range(1,len(self.D_DIMS)):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = self.layer_norm_layers[i](h)\n",
    "            x += h\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training function\n",
    "def train(modeln, parameter_dict):\n",
    "    checkpoint_directory = \"training_checkpoints_emrwgan_\"+modeln\n",
    "    # checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "    checkpoint_prefix = './training/GAN_training/' + checkpoint_directory + \"/ckpt-\"\n",
    "    data = np.array(pd.read_csv(parameter_dict['training_data_path']).values).astype('float32')\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices(data).shuffle(10000,reshuffle_each_iteration=True).batch(parameter_dict['batchsize'], drop_remainder=True)\n",
    "\n",
    "    generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=2e-5)\n",
    "\n",
    "    generator = Generator(parameter_dict)\n",
    "    discriminator = Discriminator(parameter_dict)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, directory='./training/GAN_training/' + checkpoint_directory, max_to_keep=50)\n",
    "\n",
    "    @tf.function\n",
    "    def d_step(real):\n",
    "        z = tf.random.normal(shape=[parameter_dict['batchsize'], parameter_dict['Z_DIM']])\n",
    "\n",
    "        epsilon = tf.random.uniform(\n",
    "            shape=[parameter_dict['batchsize'], 1],\n",
    "            minval=0.,\n",
    "            maxval=1.)\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            synthetic = generator(z, False)\n",
    "            interpolate = real + epsilon * (synthetic - real)\n",
    "\n",
    "            real_output = discriminator(real)\n",
    "            fake_output = discriminator(synthetic)\n",
    "\n",
    "            w_distance = (-tf.reduce_mean(real_output) + tf.reduce_mean(fake_output))\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch(interpolate)\n",
    "                interpolate_output = discriminator(interpolate)\n",
    "            w_grad = t.gradient(interpolate_output, interpolate)\n",
    "            slopes = tf.sqrt(tf.reduce_sum(tf.square(w_grad), 1))\n",
    "            gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "\n",
    "            disc_loss = 10 * gradient_penalty + w_distance\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        return disc_loss, w_distance\n",
    "\n",
    "    @tf.function\n",
    "    def g_step():\n",
    "        z = tf.random.normal(shape=[parameter_dict['batchsize'], parameter_dict['Z_DIM']])\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            synthetic = generator(z,True)\n",
    "\n",
    "            fake_output = discriminator(synthetic)\n",
    "\n",
    "            gen_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(batch):\n",
    "        disc_loss, w_distance = d_step(batch)\n",
    "        g_step()\n",
    "        return disc_loss, w_distance\n",
    "\n",
    "    print('training start', flush=True)\n",
    "\n",
    "    best_loss = 1000000.0\n",
    "    for epoch in range(15000):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        total_w = 0.0\n",
    "        step = 0.0\n",
    "        for args in dataset_train:\n",
    "            loss, w = train_step(args)\n",
    "            total_loss += loss\n",
    "            total_w += w\n",
    "            step += 1\n",
    "        duration_epoch = time.time() - start_time\n",
    "        format_str = 'epoch: %d, loss = %f, w = %f, (%.2f)'\n",
    "        if epoch % 10 == 0:\n",
    "            print(format_str % (epoch, -total_loss / step, -total_w / step, duration_epoch), flush=True)\n",
    "            if epoch > 100 and epoch % 50 == 0 and -total_loss / step <= best_loss and -total_loss / step > 0:\n",
    "                best_loss = -total_loss / step\n",
    "                manager.save(checkpoint_number=epoch)\n",
    "                print('ckpt %d saved with loss %.6f' % (epoch, best_loss), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=str)\n",
    "parser.add_argument('--model_id', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "parameter_dict = {}\n",
    "parameter_dict['training_data_path'] = './Data/preprocessing/test/normalized_training_data.csv'\n",
    "parameter_dict['feature_range_path'] = './Data/preprocessing/test/min_max_log.npy'\n",
    "parameter_dict['continuous_feature_col_ind'] = [1456,1457,1458,1459]\n",
    "parameter_dict['batchsize'] = 4096\n",
    "parameter_dict['Z_DIM'] = 128\n",
    "parameter_dict['dimension'] = 1460\n",
    "parameter_dict['h_dimension'] = 384\n",
    "parameter_dict['race_dimension'] = 6\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "train(args.model_id, parameter_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
