{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701eef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97830946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob2onehot(prob):\n",
    "    return tf.cast((tf.reduce_max(prob, axis=-1, keepdims=True) - prob) == 0, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator NN for GAN\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, parameter_dict):\n",
    "        super(Generator, self).__init__()\n",
    "        self.G_DIMS = [parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['dimension']-parameter_dict['race_dimension']]\n",
    "        self.dense_layers = [tf.keras.layers.Dense(dim) for dim in self.G_DIMS[:-1]]\n",
    "        self.batch_norm_layers = [tf.keras.layers.BatchNormalization(epsilon=1e-5) for _ in self.G_DIMS[:-1]]\n",
    "        self.output_layer_code = tf.keras.layers.Dense(self.G_DIMS[-1], activation=tf.nn.sigmoid)\n",
    "        self.output_layer_race = tf.keras.layers.Dense(parameter_dict['race_dimension'], activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        h = self.dense_layers[0](x)\n",
    "        x = tf.nn.relu(self.batch_norm_layers[0](h, training=training))\n",
    "        for i in range(1,len(self.G_DIMS[:-1])):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = tf.nn.relu(self.batch_norm_layers[i](h, training=training))\n",
    "            x += h\n",
    "        x = tf.concat((self.output_layer_race(x), self.output_layer_code(x)),axis=-1)\n",
    "        return x\n",
    "\n",
    "    def test(self, x):\n",
    "        h = self.dense_layers[0](x)\n",
    "        x = tf.nn.relu(self.batch_norm_layers[0](h, training=False))\n",
    "        for i in range(1,len(self.G_DIMS[:-1])):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = tf.nn.relu(self.batch_norm_layers[i](h, training=False))\n",
    "            x += h\n",
    "        x = tf.concat((prob2onehot(self.output_layer_race(x)), self.output_layer_code(x)),axis=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator NN for GAN\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, parameter_dict):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.D_DIMS = [parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension']]\n",
    "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.relu) for dim in self.D_DIMS]\n",
    "        self.layer_norm_layers = [tf.keras.layers.LayerNormalization(epsilon=1e-5) for _ in self.D_DIMS]\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense_layers[0](x)\n",
    "        x = self.layer_norm_layers[0](x)\n",
    "        for i in range(1,len(self.D_DIMS)):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = self.layer_norm_layers[i](h)\n",
    "            x += h\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training function\n",
    "def train(modeln, parameter_dict):\n",
    "    checkpoint_directory = \"training_checkpoints_emrwgan_\"+modeln\n",
    "    # checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "    checkpoint_prefix = './training/GAN_training/' + checkpoint_directory + \"/ckpt-\"\n",
    "    data = np.array(pd.read_csv(parameter_dict['training_data_path']).values).astype('float32')\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices(data).shuffle(10000,reshuffle_each_iteration=True).batch(parameter_dict['batchsize'], drop_remainder=True)\n",
    "\n",
    "    generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=2e-5)\n",
    "\n",
    "    generator = Generator(parameter_dict)\n",
    "    discriminator = Discriminator(parameter_dict)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, directory='./training/GAN_training/' + checkpoint_directory, max_to_keep=50)\n",
    "\n",
    "    @tf.function\n",
    "    def d_step(real):\n",
    "        z = tf.random.normal(shape=[parameter_dict['batchsize'], parameter_dict['Z_DIM']])\n",
    "\n",
    "        epsilon = tf.random.uniform(\n",
    "            shape=[parameter_dict['batchsize'], 1],\n",
    "            minval=0.,\n",
    "            maxval=1.)\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            synthetic = generator(z, False)\n",
    "            interpolate = real + epsilon * (synthetic - real)\n",
    "\n",
    "            real_output = discriminator(real)\n",
    "            fake_output = discriminator(synthetic)\n",
    "\n",
    "            w_distance = (-tf.reduce_mean(real_output) + tf.reduce_mean(fake_output))\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch(interpolate)\n",
    "                interpolate_output = discriminator(interpolate)\n",
    "            w_grad = t.gradient(interpolate_output, interpolate)\n",
    "            slopes = tf.sqrt(tf.reduce_sum(tf.square(w_grad), 1))\n",
    "            gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "\n",
    "            disc_loss = 10 * gradient_penalty + w_distance\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        return disc_loss, w_distance\n",
    "\n",
    "    @tf.function\n",
    "    def g_step():\n",
    "        z = tf.random.normal(shape=[parameter_dict['batchsize'], parameter_dict['Z_DIM']])\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            synthetic = generator(z,True)\n",
    "\n",
    "            fake_output = discriminator(synthetic)\n",
    "\n",
    "            gen_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(batch):\n",
    "        disc_loss, w_distance = d_step(batch)\n",
    "        g_step()\n",
    "        return disc_loss, w_distance\n",
    "\n",
    "    print('training start', flush=True)\n",
    "\n",
    "    best_loss = 1000000.0\n",
    "    for epoch in range(15000):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        total_w = 0.0\n",
    "        step = 0.0\n",
    "        for args in dataset_train:\n",
    "            loss, w = train_step(args)\n",
    "            total_loss += loss\n",
    "            total_w += w\n",
    "            step += 1\n",
    "        duration_epoch = time.time() - start_time\n",
    "        format_str = 'epoch: %d, loss = %f, w = %f, (%.2f)'\n",
    "        if epoch % 10 == 0:\n",
    "            print(format_str % (epoch, -total_loss / step, -total_w / step, duration_epoch), flush=True)\n",
    "            if epoch > 100 and epoch % 50 == 0 and -total_loss / step <= best_loss and -total_loss / step > 0:\n",
    "                best_loss = -total_loss / step\n",
    "                manager.save(checkpoint_number=epoch)\n",
    "                print('ckpt %d saved with loss %.6f' % (epoch, best_loss), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=str)\n",
    "parser.add_argument('--model_id', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "parameter_dict = {}\n",
    "parameter_dict['training_data_path'] = './Data/preprocessing/test/normalized_training_data.csv'\n",
    "parameter_dict['feature_range_path'] = './Data/preprocessing/test/min_max_log.npy'\n",
    "parameter_dict['continuous_feature_col_ind'] = [1456,1457,1458,1459]\n",
    "parameter_dict['batchsize'] = 4096\n",
    "parameter_dict['Z_DIM'] = 128\n",
    "parameter_dict['dimension'] = 1460\n",
    "parameter_dict['h_dimension'] = 384\n",
    "parameter_dict['race_dimension'] = 6\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "train(args.model_id, parameter_dict)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
