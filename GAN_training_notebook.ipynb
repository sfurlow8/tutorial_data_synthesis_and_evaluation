{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0511634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount to my Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc9ecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 20 00:43:12 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d9c300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) # should be true\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0)) # should be NVIDIA A100-SXM4-40GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33dc73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'tutorial_data_synthesis_and_evaluation'...\n",
      "remote: Enumerating objects: 197, done.\u001b[K\n",
      "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
      "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
      "remote: Total 197 (delta 39), reused 53 (delta 14), pack-reused 96 (from 1)\u001b[K\n",
      "Receiving objects: 100% (197/197), 30.87 MiB | 16.90 MiB/s, done.\n",
      "Resolving deltas: 100% (92/92), done.\n",
      "/content/tutorial_data_synthesis_and_evaluation\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/sfurlow8/tutorial_data_synthesis_and_evaluation.git\n",
    "%cd tutorial_data_synthesis_and_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab379544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.0,>=1.26 (from -r requirements_colab.txt (line 6))\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14,>=1.11 (from -r requirements_colab.txt (line 7))\n",
      "  Using cached scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting pandas<2.3,>=2.0 (from -r requirements_colab.txt (line 8))\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib<3.10,>=3.8 (from -r requirements_colab.txt (line 9))\n",
      "  Using cached matplotlib-3.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scikit-learn<1.6,>=1.3 (from -r requirements_colab.txt (line 10))\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting lightgbm<4.7,>=4.0 (from -r requirements_colab.txt (line 13))\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Collecting joblib<2.0,>=1.2 (from -r requirements_colab.txt (line 14))\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting shap<0.46,>=0.42 (from -r requirements_colab.txt (line 15))\n",
      "  Using cached shap-0.45.1-cp312-cp312-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Collecting requests<2.33,>=2.28 (from -r requirements_colab.txt (line 16))\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas<2.3,>=2.0->-r requirements_colab.txt (line 8))\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas<2.3,>=2.0->-r requirements_colab.txt (line 8))\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<2.3,>=2.0->-r requirements_colab.txt (line 8))\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<3.10,>=3.8->-r requirements_colab.txt (line 9))\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<3.10,>=3.8->-r requirements_colab.txt (line 9))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<3.10,>=3.8->-r requirements_colab.txt (line 9))\n",
      "  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib<3.10,>=3.8->-r requirements_colab.txt (line 9))\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib<3.10,>=3.8->-r requirements_colab.txt (line 9))\n",
      "  Downloading packaging-26.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib<3.10,>=3.8->-r requirements_colab.txt (line 9))\n",
      "  Downloading pillow-12.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib<3.10,>=3.8->-r requirements_colab.txt (line 9)) (2.4.7)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<1.6,>=1.3->-r requirements_colab.txt (line 10))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tqdm>=4.27.0 (from shap<0.46,>=0.42->-r requirements_colab.txt (line 15))\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting slicer==0.0.8 (from shap<0.46,>=0.42->-r requirements_colab.txt (line 15))\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba (from shap<0.46,>=0.42->-r requirements_colab.txt (line 15))\n",
      "  Downloading numba-0.64.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting cloudpickle (from shap<0.46,>=0.42->-r requirements_colab.txt (line 15))\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<2.33,>=2.28->-r requirements_colab.txt (line 16))\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<2.33,>=2.28->-r requirements_colab.txt (line 16))\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<2.33,>=2.28->-r requirements_colab.txt (line 16))\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<2.33,>=2.28->-r requirements_colab.txt (line 16))\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.0->-r requirements_colab.txt (line 8)) (1.16.0)\n",
      "Collecting llvmlite<0.47,>=0.46.0dev0 (from numba->shap<0.46,>=0.42->-r requirements_colab.txt (line 15))\n",
      "  Downloading llvmlite-0.46.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m147.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached matplotlib-3.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached shap-0.45.1-cp312-cp312-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (544 kB)\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.6/362.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m149.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-26.0-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading numba-0.64.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.46.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, urllib3, tzdata, tqdm, threadpoolctl, slicer, python-dateutil, pillow, packaging, numpy, llvmlite, kiwisolver, joblib, idna, fonttools, cycler, cloudpickle, charset_normalizer, certifi, scipy, requests, pandas, numba, contourpy, scikit-learn, matplotlib, lightgbm, shap\n",
      "Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 cloudpickle-3.1.2 contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 idna-3.11 joblib-1.5.3 kiwisolver-1.4.9 lightgbm-4.6.0 llvmlite-0.46.0 matplotlib-3.9.4 numba-0.64.0 numpy-1.26.4 packaging-26.0 pandas-2.2.3 pillow-12.1.1 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.32.5 scikit-learn-1.5.2 scipy-1.13.1 shap-0.45.1 slicer-0.0.8 threadpoolctl-3.6.0 tqdm-4.67.3 tzdata-2025.3 urllib3-2.6.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "69dc279d518d4260826fe910a6558909",
       "pip_warning": {
        "packages": [
         "PIL",
         "certifi",
         "cycler",
         "dateutil",
         "idna",
         "kiwisolver",
         "matplotlib",
         "mpl_toolkits",
         "numpy",
         "packaging",
         "requests",
         "scipy",
         "tqdm",
         "urllib3"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r requirements_colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52592ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.15.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0, 2.21.0rc0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow 2.16.2 (earliest stable version supporting Python 3.12)\n",
    "# Install compatible protobuf version first to avoid symbol mismatch errors\n",
    "!pip install \"protobuf==4.25.3\" --force-reinstall\n",
    "!pip install \"tensorflow==2.16.2\"\n",
    "!pip install \"protobuf==4.25.3\" --force-reinstall --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "701eef1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3758125790.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97830946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob2onehot(prob):\n",
    "    return tf.cast((tf.reduce_max(prob, axis=-1, keepdims=True) - prob) == 0, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator NN for GAN\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, parameter_dict):\n",
    "        super(Generator, self).__init__()\n",
    "        self.G_DIMS = [parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['dimension']-parameter_dict['race_dimension']]\n",
    "        self.dense_layers = [tf.keras.layers.Dense(dim) for dim in self.G_DIMS[:-1]]\n",
    "        self.batch_norm_layers = [tf.keras.layers.BatchNormalization(epsilon=1e-5) for _ in self.G_DIMS[:-1]]\n",
    "        self.output_layer_code = tf.keras.layers.Dense(self.G_DIMS[-1], activation=tf.nn.sigmoid)\n",
    "        self.output_layer_race = tf.keras.layers.Dense(parameter_dict['race_dimension'], activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        h = self.dense_layers[0](x)\n",
    "        x = tf.nn.relu(self.batch_norm_layers[0](h, training=training))\n",
    "        for i in range(1,len(self.G_DIMS[:-1])):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = tf.nn.relu(self.batch_norm_layers[i](h, training=training))\n",
    "            x += h\n",
    "        x = tf.concat((self.output_layer_race(x), self.output_layer_code(x)),axis=-1)\n",
    "        return x\n",
    "\n",
    "    def test(self, x):\n",
    "        h = self.dense_layers[0](x)\n",
    "        x = tf.nn.relu(self.batch_norm_layers[0](h, training=False))\n",
    "        for i in range(1,len(self.G_DIMS[:-1])):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = tf.nn.relu(self.batch_norm_layers[i](h, training=False))\n",
    "            x += h\n",
    "        x = tf.concat((prob2onehot(self.output_layer_race(x)), self.output_layer_code(x)),axis=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator NN for GAN\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, parameter_dict):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.D_DIMS = [parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension'], parameter_dict['h_dimension']]\n",
    "        self.dense_layers = [tf.keras.layers.Dense(dim, activation=tf.nn.relu) for dim in self.D_DIMS]\n",
    "        self.layer_norm_layers = [tf.keras.layers.LayerNormalization(epsilon=1e-5) for _ in self.D_DIMS]\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense_layers[0](x)\n",
    "        x = self.layer_norm_layers[0](x)\n",
    "        for i in range(1,len(self.D_DIMS)):\n",
    "            h = self.dense_layers[i](x)\n",
    "            h = self.layer_norm_layers[i](h)\n",
    "            x += h\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training function\n",
    "def train(modeln, parameter_dict):\n",
    "    checkpoint_directory = \"training_checkpoints_emrwgan_\"+modeln\n",
    "    # checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "    checkpoint_prefix = './training/GAN_training/' + checkpoint_directory + \"/ckpt-\"\n",
    "    data = np.array(pd.read_csv(parameter_dict['training_data_path']).values).astype('float32')\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices(data).shuffle(10000,reshuffle_each_iteration=True).batch(parameter_dict['batchsize'], drop_remainder=True)\n",
    "\n",
    "    generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=2e-5)\n",
    "\n",
    "    generator = Generator(parameter_dict)\n",
    "    discriminator = Discriminator(parameter_dict)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, directory='./training/GAN_training/' + checkpoint_directory, max_to_keep=50)\n",
    "\n",
    "    @tf.function\n",
    "    def d_step(real):\n",
    "        z = tf.random.normal(shape=[parameter_dict['batchsize'], parameter_dict['Z_DIM']])\n",
    "\n",
    "        epsilon = tf.random.uniform(\n",
    "            shape=[parameter_dict['batchsize'], 1],\n",
    "            minval=0.,\n",
    "            maxval=1.)\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            synthetic = generator(z, False)\n",
    "            interpolate = real + epsilon * (synthetic - real)\n",
    "\n",
    "            real_output = discriminator(real)\n",
    "            fake_output = discriminator(synthetic)\n",
    "\n",
    "            w_distance = (-tf.reduce_mean(real_output) + tf.reduce_mean(fake_output))\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch(interpolate)\n",
    "                interpolate_output = discriminator(interpolate)\n",
    "            w_grad = t.gradient(interpolate_output, interpolate)\n",
    "            slopes = tf.sqrt(tf.reduce_sum(tf.square(w_grad), 1))\n",
    "            gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "\n",
    "            disc_loss = 10 * gradient_penalty + w_distance\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        return disc_loss, w_distance\n",
    "\n",
    "    @tf.function\n",
    "    def g_step():\n",
    "        z = tf.random.normal(shape=[parameter_dict['batchsize'], parameter_dict['Z_DIM']])\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            synthetic = generator(z,True)\n",
    "\n",
    "            fake_output = discriminator(synthetic)\n",
    "\n",
    "            gen_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(batch):\n",
    "        disc_loss, w_distance = d_step(batch)\n",
    "        g_step()\n",
    "        return disc_loss, w_distance\n",
    "\n",
    "    print('training start', flush=True)\n",
    "\n",
    "    best_loss = 1000000.0\n",
    "    for epoch in range(15000):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        total_w = 0.0\n",
    "        step = 0.0\n",
    "        for args in dataset_train:\n",
    "            loss, w = train_step(args)\n",
    "            total_loss += loss\n",
    "            total_w += w\n",
    "            step += 1\n",
    "        duration_epoch = time.time() - start_time\n",
    "        format_str = 'epoch: %d, loss = %f, w = %f, (%.2f)'\n",
    "        if epoch % 10 == 0:\n",
    "            print(format_str % (epoch, -total_loss / step, -total_w / step, duration_epoch), flush=True)\n",
    "            if epoch > 100 and epoch % 50 == 0 and -total_loss / step <= best_loss and -total_loss / step > 0:\n",
    "                best_loss = -total_loss / step\n",
    "                manager.save(checkpoint_number=epoch)\n",
    "                print('ckpt %d saved with loss %.6f' % (epoch, best_loss), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=str)\n",
    "parser.add_argument('--model_id', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "parameter_dict = {}\n",
    "parameter_dict['training_data_path'] = './Data/preprocessing/test/normalized_training_data.csv'\n",
    "parameter_dict['feature_range_path'] = './Data/preprocessing/test/min_max_log.npy'\n",
    "parameter_dict['continuous_feature_col_ind'] = [1456,1457,1458,1459]\n",
    "parameter_dict['batchsize'] = 4096\n",
    "parameter_dict['Z_DIM'] = 128\n",
    "parameter_dict['dimension'] = 1460\n",
    "parameter_dict['h_dimension'] = 384\n",
    "parameter_dict['race_dimension'] = 6\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "train(args.model_id, parameter_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
